{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7d2e3e-290f-4d9b-be0d-a76892618809",
   "metadata": {},
   "source": [
    "# Sentiment Classification with LoRA  Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0af6c1-7b28-48d6-b50b-502cb5886ca4",
   "metadata": {},
   "source": [
    "## Pre-Check: \n",
    "\n",
    "Before running the project, verify the following technologies are installed and accessible:\n",
    "\n",
    "## Environment\n",
    "- Python 3.9+  \n",
    "- Jupyter Notebook or JupyterLab  \n",
    "\n",
    "## Core Libraries\n",
    "- torch (PyTorch)  \n",
    "- transformers (Hugging Face)  \n",
    "- peft (Hugging Face PEFT for LoRA)  \n",
    "- datasets (Hugging Face Datasets)  \n",
    "- scikit-learn  \n",
    "- numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387afdf2-2f25-4a83-a5d7-b46df0a3e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Pre-Check ===\n",
      "Torch version:          2.9.1+cpu\n",
      "Transformers version:   4.57.3\n",
      "PEFT version:           0.18.0\n",
      "Datasets version:       4.4.1\n",
      "Scikit-learn version:   1.7.2\n",
      "NumPy version:          2.3.5\n",
      "Evaluate version: 0.4.6\n",
      "\n",
      "=== Device Check ===\n",
      "CUDA available: False\n",
      "Device in use:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Device Check\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "print(\"=== Environment Pre-Check ===\")\n",
    "print(f\"Torch version:          {torch.__version__}\")\n",
    "print(f\"Transformers version:   {transformers.__version__}\")\n",
    "print(f\"PEFT version:           {peft.__version__}\")\n",
    "print(f\"Datasets version:       {datasets.__version__}\")\n",
    "print(f\"Scikit-learn version:   {sklearn.__version__}\")\n",
    "print(f\"NumPy version:          {np.__version__}\")\n",
    "print(f\"Evaluate version: {evaluate.__version__}\")\n",
    "\n",
    "print(\"\\n=== Device Check ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device in use:  {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cb222-9ae7-4923-8c98-77d8e41fb24a",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29866fe3-3c1b-42e8-8593-527779f6a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load IMDb dataset\n",
    "raw = load_dataset(\"imdb\")\n",
    "\n",
    "# Create validation split from train\n",
    "splits = raw[\"train\"].train_test_split(test_size=0.2, stratify_by_column=\"label\", seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "test_ds = raw[\"test\"]\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n",
    "\n",
    "# ‚ö° For CPU debugging, shrink dataset\n",
    "dataset_small = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].select(range(500)),\n",
    "    \"validation\": dataset[\"validation\"].select(range(200)),\n",
    "    \"test\": dataset[\"test\"].select(range(100))\n",
    "})\n",
    "dataset = dataset_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa0681-fb34-4dee-96c4-78e0cba0fa1b",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "- Load IMDb dataset from Hugging Face.  \n",
    "- Create a stratified validation split.  \n",
    "- Shrink dataset for CPU debugging:  \n",
    "  - Train: 500  \n",
    "  - Validation: 200  \n",
    "  - Test: 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be811385-a33e-468e-bd2e-64876f405294",
   "metadata": {},
   "source": [
    "## 2. Tokenization & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aeb7d18-5dbd-4285-ba88-a6dbc060d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    inputs = [f\"review: {t}\" for t in examples[\"text\"]]\n",
    "    labels_text = [\"negative\" if l == 0 else \"positive\" for l in examples[\"label\"]]\n",
    "    enc = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    enc_targets = tokenizer(text_target=labels_text, max_length=5, truncation=True, padding=\"max_length\")\n",
    "    enc[\"labels\"] = enc_targets[\"input_ids\"]\n",
    "    return enc\n",
    "\n",
    "tokenized = dataset.map(preprocess_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fb57c-3ff2-4201-becb-434dbf47d98c",
   "metadata": {},
   "source": [
    "## Tokenization & Formatting\n",
    "- Inputs: \"review: <text>\"\n",
    "- Labels: \"positive\" / \"negative\"\n",
    "   - Tokenized into IDs with truncation and padding.\n",
    "   - Labels tokenized via text_target; max_length set appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b315ccb-74b9-4a76-b51f-7b6d2df0b5b0",
   "metadata": {},
   "source": [
    "## 3. Baseline Comparison (No Fine‚ÄëTuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4808238a-2cc4-486f-a132-ec7de39dcf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (no fine-tuning) on 200 validation samples:\n",
      "Accuracy: 0.49\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\anaconda3\\envs\\llm-finetune\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "from transformers import pipeline\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "baseline_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=baseline_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "def baseline_predict(texts):\n",
    "    prompts = [f\"review: {t}\" for t in texts]\n",
    "    # Tokenize with truncation to avoid >512 tokens\n",
    "    enc = tokenizer(prompts, max_length=256, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "    outs = baseline_model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "        max_new_tokens=3\n",
    "    )\n",
    "    preds_str = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "    return [1 if \"positive\" in s.lower() else 0 for s in preds_str]\n",
    "\n",
    "sample = dataset[\"validation\"].select(range(200))\n",
    "baseline_preds = baseline_predict(sample[\"text\"])\n",
    "baseline_refs = sample[\"label\"]\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "print(\"\\nBaseline (no fine-tuning) on 200 validation samples:\")\n",
    "print(\"Accuracy:\", accuracy.compute(predictions=baseline_preds, references=baseline_refs)[\"accuracy\"])\n",
    "print(\"Precision:\", precision.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"precision\"])\n",
    "print(\"Recall:\", recall.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"recall\"])\n",
    "print(\"F1:\", f1.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91935bc1-76f3-4303-b2e6-dfe0effcc6ad",
   "metadata": {},
   "source": [
    "## 4. PEFT + LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b24f60-0cba-4b5a-9de2-07268532490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9d158-a4ea-4059-b568-3741a856ecb0",
   "metadata": {},
   "source": [
    "### üß© PEFT + LoRA\n",
    "- Load **T5‚Äësmall** (~60M params).  \n",
    "- Apply LoRA to attention projections (`q`, `v`).  \n",
    "- Trainable params <1% of total ‚Üí efficient fine‚Äëtuning.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d0ae-a6f6-453c-98bf-93726c30b7ba",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6279a6c-82f5-4123-9c6c-ed1def71b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\AppData\\Local\\Temp\\ipykernel_26912\\4087586163.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.075764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\anaconda3\\envs\\llm-finetune\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.14092777252197267, metrics={'train_runtime': 212.9274, 'train_samples_per_second': 2.348, 'train_steps_per_second': 0.587, 'total_flos': 17030971392000.0, 'train_loss': 0.14092777252197267, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4227dce-2f7c-45cb-be44-86968ec73732",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2acb7bd7-e596-4859-b4b2-632cfbf4a8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\anaconda3\\envs\\llm-finetune\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "Accuracy: 0.67\n",
      "Precision: 0.9736842105263158\n",
      "Recall: 0.3627450980392157\n",
      "F1: 0.5285714285714286\n",
      "\n",
      "Classification report (validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.99      0.75        98\n",
      "    positive       0.97      0.36      0.53       102\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.79      0.68      0.64       200\n",
      "weighted avg       0.79      0.67      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def to_int_labels(strs):\n",
    "    return [1 if \"positive\" in s.lower() else 0 for s in strs]\n",
    "\n",
    "# Validation evaluation\n",
    "val_out = trainer.predict(tokenized[\"validation\"])\n",
    "val_preds_str = tokenizer.batch_decode(val_out.predictions, skip_special_tokens=True)\n",
    "val_labels_str = tokenizer.batch_decode(val_out.label_ids, skip_special_tokens=True)\n",
    "\n",
    "val_preds = to_int_labels(val_preds_str)\n",
    "val_refs = to_int_labels(val_labels_str)\n",
    "\n",
    "print(\"Validation metrics:\")\n",
    "print(\"Accuracy:\", accuracy.compute(predictions=val_preds, references=val_refs)[\"accuracy\"])\n",
    "print(\"Precision:\", precision.compute(predictions=val_preds, references=val_refs, average=\"binary\")[\"precision\"])\n",
    "print(\"Recall:\", recall.compute(predictions=val_preds, references=val_refs, average=\"binary\")[\"recall\"])\n",
    "print(\"F1:\", f1.compute(predictions=val_preds, references=val_refs, average=\"binary\")[\"f1\"])\n",
    "\n",
    "print(\"\\nClassification report (validation):\")\n",
    "print(classification_report(val_refs, val_preds, target_names=[\"negative\", \"positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf8ad7-d124-4fc0-bbb6-9c469201d4f2",
   "metadata": {},
   "source": [
    "### üìä Validation Results\n",
    "- **Accuracy: 0.67**  \n",
    "  The model correctly classified about two‚Äëthirds of the validation reviews, showing improvement over the baseline.\n",
    "\n",
    "- **Precision: 0.97**  \n",
    "  Predictions of ‚Äúpositive‚Äù are almost always correct, meaning the model is highly reliable when it chooses that label.\n",
    "\n",
    "- **Recall: 0.36**  \n",
    "  The model misses many actual positives, defaulting to ‚Äúnegative‚Äù more often than it should.\n",
    "\n",
    "- **F1 Score: 0.53**  \n",
    "  Reflects the trade‚Äëoff: excellent precision but weaker recall for positives.\n",
    "\n",
    "- **Class Breakdown:**  \n",
    "  - Negative ‚Üí Precision: 0.60, Recall: 0.99, F1: 0.75  \n",
    "  - Positive ‚Üí Precision: 0.97, Recall: 0.36, F1: 0.53  \n",
    "\n",
    "**In short:** LoRA fine‚Äëtuning turned the baseline into a usable classifier ‚Äî strong at catching negatives, very precise with positives, but still limited in recall for positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356899c9-6a14-4f27-a4a7-c1ce7df97339",
   "metadata": {},
   "source": [
    "## 7. Inference on Custom Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585fc952-4a53-46f2-ba83-93fa9213428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: positive\n",
      "Test 2: negative\n",
      "Test 3: negative\n"
     ]
    }
   ],
   "source": [
    "def classify_review(text: str):\n",
    "    prompt = f\"review: {text}\"\n",
    "    gen = trainer.model.generate(**tokenizer(prompt, return_tensors=\"pt\"), max_new_tokens=3)\n",
    "    pred_str = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    return \"positive\" if \"positive\" in pred_str.lower() else \"negative\"\n",
    "\n",
    "# Clear positive case\n",
    "print(\"Test 1:\", classify_review(\"This movie was amazing!\"))  # Expected: positive\n",
    "\n",
    "# Clear negative case\n",
    "print(\"Test 2:\", classify_review(\"Terrible acting and a boring plot.\"))  # Expected: negative\n",
    "\n",
    "# Ambiguous/mixed sentiment case\n",
    "print(\"Test 3:\", classify_review(\"The visuals were stunning, but the story was weak.\"))  # Model‚Äôs prediction may vary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12faa7-3477-400d-8aaf-aa061b9ecafe",
   "metadata": {},
   "source": [
    "### Inference\n",
    "- Test model on custom reviews.  \n",
    "- Outputs `\"positive\"` or `\"negative\"`.  \n",
    "- **Test 1:** \"This movie was amazing!\" ‚Üí Output: positive  \n",
    "- **Test 2:** \"Terrible acting and a boring plot.\" ‚Üí Output: negative  \n",
    "- **Test 3:** \"The visuals were stunning, but the story was weak.\" ‚Üí Output: negative  \n",
    "\n",
    "The model correctly handled clear sentiment cases. For mixed reviews, it leaned toward negative, consistent with the lower recall for positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322f7bb-7521-4bfc-9813-cf2f3323bfaf",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb1845-ad49-40cc-8597-b1fb9fff9301",
   "metadata": {},
   "source": [
    "- **Baseline:** Failed to recognize positives, predicting only negatives.  \n",
    "- **LoRA Fine‚ÄëTuning:** Improved performance with measurable gains in accuracy and F1 score.  \n",
    "- **Strengths:** High precision for positives, strong recall for negatives.  \n",
    "- **Limitations:** Positive recall remains low; the model struggles with mixed or nuanced sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4089c9b-95a9-4af8-87d1-58b6070a0104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-finetune)",
   "language": "python",
   "name": "llm-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
